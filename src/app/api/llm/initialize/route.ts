import { NextRequest, NextResponse } from 'next/server';
import { initializeLLM, isLlamaServerRunning, shutdownLLM } from '@/lib/gemma';
import { pingLlamaServer, testServerConnection } from '@/lib/gemma/llama-client';

// ã‚µãƒ¼ãƒãƒ¼èµ·å‹•çŠ¶æ…‹ã®è¿½è·¡
let isServerStarting = false;
let lastInitTime = 0;

/**
 * LLMã‚µãƒ¼ãƒãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
 * POSTãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã¾ã™
 */
export async function POST(req: NextRequest) {
  console.log('ğŸŸ¢ [API Route] POST /api/llm/initialize received');
  
  try {
    // ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã‚’è§£æ
    const body = await req.json();
    const autoStart = body.autoStart === true;
    const forceRestart = body.forceRestart === true;
    
    console.log(`ğŸŸ¢ [API Route] autoStart=${autoStart}, forceRestart=${forceRestart}`);
    
    // ã‚µãƒ¼ãƒãƒ¼ãŒæ—¢ã«å‹•ä½œã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    let serverRunning = false;
    let serverStatusDetail = 'unknown';
    
    // ã¾ãšãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
    if (isLlamaServerRunning()) {
      console.log('ğŸŸ¢ [API Route] LLM server process is already running');
      serverRunning = true;
      serverStatusDetail = 'process_running';
    } else {
      console.log('ğŸŸ¢ [API Route] LLM server process is not running');
      serverStatusDetail = 'process_stopped';
    }
    
    // æ¬¡ã«HTTPãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯ï¼ˆåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§èµ·å‹•ã—ã¦ã„ã‚‹å ´åˆã‚‚æ¤œå‡ºï¼‰
    try {
      const connectionTest = await testServerConnection();
      if (connectionTest.success) {
        console.log(`ğŸŸ¢ [API Route] LLM server is responding to HTTP requests (${connectionTest.status})`);
        serverRunning = true;
        serverStatusDetail = connectionTest.status === 'initializing' ? 'initializing' : 'ready';
      } else {
        console.log(`ğŸŸ¢ [API Route] LLM server is not responding to HTTP requests (${connectionTest.status})`);
        serverStatusDetail = 'not_responding';
      }
    } catch (pingError) {
      console.warn('ğŸŸ¡ [API Route] Error testing LLM server connection:', pingError);
    }
    
    // å¼·åˆ¶å†èµ·å‹•ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã€æ—¢å­˜ã‚µãƒ¼ãƒãƒ¼ã‚’åœæ­¢
    if (forceRestart && serverRunning) {
      console.log('ğŸŸ¢ [API Route] Force restart requested, shutting down existing server');
      try {
        await shutdownLLM();
        serverRunning = false;
        serverStatusDetail = 'shutdown_for_restart';
        
        // å°‘ã—å¾…æ©Ÿã—ã¦å®Œå…¨ã«çµ‚äº†ã™ã‚‹ã®ã‚’å¾…ã¤
        await new Promise(resolve => setTimeout(resolve, 2000));
      } catch (shutdownError) {
        console.error('ğŸ”´ [API Route] Error shutting down LLM server:', shutdownError);
        return NextResponse.json({ 
          success: false, 
          error: 'Failed to shutdown existing server',
          detail: shutdownError instanceof Error ? shutdownError.message : String(shutdownError)
        }, { status: 500 });
      }
    }
    
    // ã‚µãƒ¼ãƒãƒ¼ãŒå®Ÿè¡Œä¸­ã§ã‚ã‚Œã°ã€åˆæœŸåŒ–ã¯ä¸è¦
    if (serverRunning && !forceRestart) {
      console.log(`ğŸŸ¢ [API Route] LLM server is already running (${serverStatusDetail}), no initialization needed`);
      return NextResponse.json({ 
        success: true, 
        initialized: false,
        status: 'already_running',
        detail: serverStatusDetail,
        message: 'LLM server is already running' 
      });
    }
    
    // ã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ä¸­ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
    if (isServerStarting) {
      // é•·æ™‚é–“èµ·å‹•ä¸­ã®å ´åˆã¯ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆ10åˆ†ä»¥ä¸ŠçµŒéï¼‰
      const now = Date.now();
      if (now - lastInitTime > 10 * 60 * 1000) {
        console.log('ğŸŸ¡ [API Route] Previous initialization seems stuck, resetting flag');
        isServerStarting = false;
      } else {
        console.log('ğŸŸ¡ [API Route] LLM server initialization is already in progress');
        return NextResponse.json({ 
          success: true, 
          initialized: false,
          status: 'initializing',
          message: 'LLM server initialization is already in progress',
          startTime: lastInitTime
        });
      }
    }
    
    // autoStart=trueã®å ´åˆã®ã¿ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•
    if (autoStart) {
      console.log('ğŸŸ¢ [API Route] Initializing LLM server...');
      
      try {
        // èµ·å‹•ä¸­ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
        isServerStarting = true;
        lastInitTime = Date.now();
        
        const initialized = await initializeLLM();
        
        // èµ·å‹•ä¸­ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        isServerStarting = false;
        
        if (initialized) {
          console.log('ğŸŸ¢ [API Route] LLM server initialized successfully');
          return NextResponse.json({ 
            success: true, 
            initialized: true,
            status: 'initialized',
            message: 'LLM server initialized successfully' 
          });
        } else {
          console.error('ğŸ”´ [API Route] LLM server initialization failed');
          return NextResponse.json({ 
            success: false, 
            error: 'Failed to initialize LLM server' 
          }, { status: 500 });
        }
      } catch (initError) {
        // ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚èµ·å‹•ä¸­ãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        isServerStarting = false;
        
        console.error('ğŸ”´ [API Route] Error initializing LLM server:', initError);
        
        const errorMessage = initError instanceof Error 
          ? initError.message 
          : 'Unknown error';
          
        return NextResponse.json({ 
          success: false, 
          error: errorMessage 
        }, { status: 500 });
      }
    } else {
      console.log('ğŸŸ¢ [API Route] Auto-start is disabled');
      return NextResponse.json({ 
        success: false, 
        initialized: false,
        status: 'not_started',
        message: 'Auto-start is disabled and LLM server is not running' 
      }, { status: 400 });
    }
  } catch (error) {
    console.error('ğŸ”´ [API Route] Error initializing LLM server:', error);
    
    const errorMessage = error instanceof Error 
      ? error.message 
      : 'Unknown error';
    
    return NextResponse.json({ 
      success: false, 
      error: errorMessage 
    }, { status: 500 });
  }
}

/**
 * LLMã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¢ºèªã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
 */
export async function GET(req: NextRequest) {
  console.log('ğŸŸ¢ [API Route] GET /api/llm/initialize received');
  
  try {
    // ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’è©³ç´°ã«ç¢ºèª
    let processRunning = isLlamaServerRunning();
    let httpResponding = false;
    let serverStarting = isServerStarting;
    let connectionDetails = { status: 'unknown', message: '' };
    
    try {
      const connectionTest = await testServerConnection();
      httpResponding = connectionTest.success;
      connectionDetails = connectionTest;
    } catch (pingError) {
      console.warn('ğŸŸ¡ [API Route] Error testing LLM server connection:', pingError);
    }
    
    // ç·åˆçš„ãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    const isRunning = processRunning || httpResponding;
    
    // çŠ¶æ…‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    let statusMessage = 'unknown';
    if (isRunning) {
      if (httpResponding) {
        statusMessage = connectionDetails.status;
      } else {
        statusMessage = 'process_running_not_responding';
      }
    } else if (serverStarting) {
      statusMessage = 'starting';
    } else {
      statusMessage = 'stopped';
    }
    
    return NextResponse.json({
      success: true,
      status: {
        processRunning,
        httpResponding,
        isRunning,
        serverStarting,
        initTime: lastInitTime,
        status: statusMessage,
        details: connectionDetails
      },
      timestamp: new Date().toISOString()
    });
  } catch (error) {
    console.error('ğŸ”´ [API Route] Error checking LLM server status:', error);
    
    const errorMessage = error instanceof Error 
      ? error.message 
      : 'Unknown error';
    
    return NextResponse.json({ 
      success: false, 
      error: errorMessage 
    }, { status: 500 });
  }
}